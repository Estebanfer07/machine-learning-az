{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CazISR8X_HUG"},"source":["# Multiple Linear Regression\n","### Construir el modelo óptimo de RLM utilizando la Eliminación hacia atrás de manera automática\n","- Como no se tiene reflejado en el dataset el `coeficiente independiente b0` entonces se procede a añadir una `columna llena de 1` (esto debido a que la librería `statsmodels` ya asume que una columna de `1` está asociada al coeficiente independiente) para que el análisis sea adecuado ya que este valor también debe formar parte de cuando se ejecute cualquier técnica de selección de datos, en este caso el de `Eliminación hacia atrás`\n","\n","- Importante tomar en cuenta que, se debe eliminar una de las columnas generadas por Dummy Variables para evitar la multicolinealidad, por lo cual se pasa el parámetro `drop_first=True` con lo cual, `X` queda solo con 2 columnas nuevas en lugar de 3"]},{"cell_type":"markdown","metadata":{},"source":["### Usando solo p-valores"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["muestra X_modeled:\n","     0          1\n","0  1.0  165349.20\n","1  1.0  162597.70\n","2  1.0  153441.51\n","3  1.0  144372.41\n","4  1.0  142107.34\n","\n","\n","   prediction       test\n","0   104667.28  103282.38\n","1   134150.83  144259.40\n","2   135207.80  146121.95\n","3    72170.54   77798.83\n","4   179090.59  191050.39\n","5   109824.77  105008.31\n","6    65644.28   81229.06\n","7   100481.43   97483.56\n","8   111431.75  110352.25\n","9   169438.15  166187.94\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","\n","def backwardElimination(x, sl):    \n","    numVars = len(x[0])    \n","    for i in range(0, numVars):        \n","        regressor_OLS = sm.OLS(y, x.tolist()).fit()        \n","        maxVar = max(regressor_OLS.pvalues).astype(float)        \n","        if maxVar > sl:            \n","            for j in range(0, numVars - i):                \n","                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    \n","                    x = np.delete(x, j, 1)    \n","    regressor_OLS.summary()    \n","    return x \n","\n","dataset = pd.read_csv('50_Startups.csv')\n","X = pd.get_dummies(dataset.iloc[:, :-1], dtype=\"int\", drop_first=True).values\n","y = dataset.iloc[:, -1].values\n","\n","SL = 0.05\n","\n","X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\n","X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n","X_Modeled = backwardElimination(X_opt, SL)\n","print(f\"muestra X_modeled:\\n{pd.DataFrame(X_Modeled[0:5,:])}\\n\\n\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_Modeled, y, test_size = 0.2, random_state = 0)\n","\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","\n","y_pred = regressor.predict(X_test)\n","\n","pd.set_option('display.precision', 2)\n","# ndarray.reshape(filas, col)\n","print(pd.DataFrame(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1), columns=[\"prediction\", \"test\"]))"]},{"cell_type":"markdown","metadata":{},"source":["### Usando p-valores y el valor de  R Cuadrado Ajustado\n","- De esta manera se tiene un criterio menos rígido a la hora de eliminar variables, por ejemplo si se tiene un p_valor de 0.06, a pesar de que cumple con que es superior a LS la diferencia es muy poca por lo que no necesariamente se debería eliminar esa variable"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["muestra X_modeled:\n","     0          1         2\n","0  1.0  165349.20  471784.0\n","1  1.0  162597.70  443898.0\n","2  1.0  153441.51  407934.0\n","3  1.0  144372.41  383199.0\n","4  1.0  142107.34  366168.0\n","\n","\n","   prediction       test\n","0   102284.66  103282.38\n","1   133873.92  144259.40\n","2   134182.14  146121.95\n","3    73701.09   77798.83\n","4   180642.25  191050.39\n","5   114717.23  105008.31\n","6    68335.08   81229.06\n","7    97433.47   97483.56\n","8   114580.92  110352.25\n","9   170343.32  166187.94\n"]}],"source":["def backwardElimination(x, SL):    \n","    numVars = len(x[0])    \n","    temp = np.zeros((50,6)).astype(int)    \n","    for i in range(0, numVars):        \n","        regressor_OLS = sm.OLS(y, x.tolist()).fit()        \n","        maxVar = max(regressor_OLS.pvalues).astype(float)        \n","        adjR_before = regressor_OLS.rsquared_adj.astype(float)        \n","        if maxVar > SL:            \n","            for j in range(0, numVars - i):                \n","                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    \n","                    temp[:,j] = x[:, j]                    \n","                    x = np.delete(x, j, 1)                    \n","                    tmp_regressor = sm.OLS(y, x.tolist()).fit()                    \n","                    adjR_after = tmp_regressor.rsquared_adj.astype(float)                    \n","                    if (adjR_before >= adjR_after):                        \n","                        x_rollback = np.hstack((x, temp[:,[0,j]]))                        \n","                        x_rollback = np.delete(x_rollback, j, 1)     \n","                        # print (regressor_OLS.summary())                        \n","                        return x_rollback                    \n","                    else:                        \n","                        continue    \n","    # regressor_OLS.summary()    \n","    return x \n"," \n","dataset = pd.read_csv('50_Startups.csv')\n","X = pd.get_dummies(dataset.iloc[:, :-1], dtype=\"int\", drop_first=True).values\n","y = dataset.iloc[:, -1].values\n","\n","SL = 0.05\n","\n","X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\n","X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n","X_Modeled = backwardElimination(X_opt, SL)\n","print(f\"muestra X_modeled:\\n{pd.DataFrame(X_Modeled[0:5,:])}\\n\\n\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_Modeled, y, test_size = 0.2, random_state = 0)\n","\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","\n","y_pred = regressor.predict(X_test)\n","\n","pd.set_option('display.precision', 2)\n","# ndarray.reshape(filas, col)\n","print(pd.DataFrame(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1), columns=[\"prediction\", \"test\"]))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPhYhte6t7H4wEK4xPpDWT7","name":"Multiple Linear Regression","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
