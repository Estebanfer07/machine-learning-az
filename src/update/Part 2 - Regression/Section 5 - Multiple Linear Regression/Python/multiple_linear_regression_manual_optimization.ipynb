{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CazISR8X_HUG"},"source":["# Multiple Linear Regression\n","### Construir el modelo óptimo de RLM utilizando la Eliminación hacia atrás\n","\n","- Como no se tiene reflejado en el dataset el `coeficiente independiente b0` entonces se procede a añadir una `columna llena de 1` (esto debido a que la librería `statsmodels` ya asume que una columna de `1` está asociada al coeficiente independiente) para que el análisis sea adecuado ya que este valor también debe formar parte de cuando se ejecute cualquier técnica de selección de datos, en este caso el de `Eliminación hacia atrás`\n","\n","- Importante tomar en cuenta que, se debe eliminar una de las columnas generadas por Dummy Variables para evitar la multicolinealidad, por lo cual se pasa el parámetro `drop_first=True` con lo cual, `X` queda solo con 2 columnas nuevas en lugar de 3"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"T_YHJjnD_Tja"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>b0</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>x3</th>\n","      <th>x4</th>\n","      <th>x5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>165349.20</td>\n","      <td>136897.80</td>\n","      <td>471784.10</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>162597.70</td>\n","      <td>151377.59</td>\n","      <td>443898.53</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>153441.51</td>\n","      <td>101145.55</td>\n","      <td>407934.54</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>144372.41</td>\n","      <td>118671.85</td>\n","      <td>383199.62</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>142107.34</td>\n","      <td>91391.77</td>\n","      <td>366168.42</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    b0         x1         x2         x3   x4   x5\n","0  1.0  165349.20  136897.80  471784.10  0.0  1.0\n","1  1.0  162597.70  151377.59  443898.53  0.0  0.0\n","2  1.0  153441.51  101145.55  407934.54  1.0  0.0\n","3  1.0  144372.41  118671.85  383199.62  0.0  1.0\n","4  1.0  142107.34   91391.77  366168.42  1.0  0.0"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","\n","dataset = pd.read_csv('50_Startups.csv')\n","X = pd.get_dummies(dataset.iloc[:, :-1], dtype=\"int\", drop_first=True).values\n","y = dataset.iloc[:, -1].values\n","\n","X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\n","SL = 0.05\n","pd.DataFrame(X[0:5, :], columns=['b0', 'x1', 'x2', 'x3', 'x4', 'x5'])"]},{"cell_type":"markdown","metadata":{},"source":["En un inicio `X_optimo` va a tomar todas las variables y se va a hacer uso de la técnica de `Mínimos cuadrados ordinales (OLS)` para la regresión ya que la librería de `sklearn` y su método `LinearRegression` no devuelven un objeto como el que esperan los métodos de `statsmodels`\n","\n","- endog => endógena => variable a predecir\n","- exog => exógena => variables independientes\n","\n","En cada paso con el método `summary()` se obtiene una tabla con varios datos, entre estos los coeficientes y el p_valor representado por `P>|t|`. Para la técnica de elimincaión hacia atrás se debe eliminar todas las variables cuyo `p_valor > SL` (una en cada paso, la que tenga el valor más alto)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.951\n","Model:                            OLS   Adj. R-squared:                  0.945\n","Method:                 Least Squares   F-statistic:                     169.9\n","Date:                Wed, 13 Sep 2023   Prob (F-statistic):           1.34e-27\n","Time:                        23:02:19   Log-Likelihood:                -525.38\n","No. Observations:                  50   AIC:                             1063.\n","Df Residuals:                      44   BIC:                             1074.\n","Df Model:                           5                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n","x1             0.8060      0.046     17.369      0.000       0.712       0.900\n","x2            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n","x3             0.0270      0.017      1.574      0.123      -0.008       0.062\n","x4           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n","x5           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n","==============================================================================\n","Omnibus:                       14.782   Durbin-Watson:                   1.283\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n","Skew:                          -0.948   Prob(JB):                     2.41e-05\n","Kurtosis:                       5.572   Cond. No.                     1.45e+06\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.45e+06. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n","regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n","print(regression_OLS.summary())"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.951\n","Model:                            OLS   Adj. R-squared:                  0.946\n","Method:                 Least Squares   F-statistic:                     217.2\n","Date:                Wed, 13 Sep 2023   Prob (F-statistic):           8.49e-29\n","Time:                        23:02:21   Log-Likelihood:                -525.38\n","No. Observations:                  50   AIC:                             1061.\n","Df Residuals:                      45   BIC:                             1070.\n","Df Model:                           4                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n","x1             0.8060      0.046     17.606      0.000       0.714       0.898\n","x2            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n","x3             0.0270      0.017      1.592      0.118      -0.007       0.061\n","x4           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n","==============================================================================\n","Omnibus:                       14.758   Durbin-Watson:                   1.282\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n","Skew:                          -0.948   Prob(JB):                     2.53e-05\n","Kurtosis:                       5.563   Cond. No.                     1.40e+06\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.4e+06. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["\n","X_opt = X[:, [0, 1, 2, 3, 4]]\n","regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n","print(regression_OLS.summary())"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.951\n","Model:                            OLS   Adj. R-squared:                  0.948\n","Method:                 Least Squares   F-statistic:                     296.0\n","Date:                Wed, 13 Sep 2023   Prob (F-statistic):           4.53e-30\n","Time:                        23:02:23   Log-Likelihood:                -525.39\n","No. Observations:                  50   AIC:                             1059.\n","Df Residuals:                      46   BIC:                             1066.\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n","x1             0.8057      0.045     17.846      0.000       0.715       0.897\n","x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n","x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n","==============================================================================\n","Omnibus:                       14.838   Durbin-Watson:                   1.282\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n","Skew:                          -0.949   Prob(JB):                     2.21e-05\n","Kurtosis:                       5.586   Cond. No.                     1.40e+06\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.4e+06. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["\n","X_opt = X[:, [0, 1, 2, 3]]\n","regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n","print(regression_OLS.summary())"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.950\n","Model:                            OLS   Adj. R-squared:                  0.948\n","Method:                 Least Squares   F-statistic:                     450.8\n","Date:                Wed, 13 Sep 2023   Prob (F-statistic):           2.16e-31\n","Time:                        23:02:25   Log-Likelihood:                -525.54\n","No. Observations:                  50   AIC:                             1057.\n","Df Residuals:                      47   BIC:                             1063.\n","Df Model:                           2                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n","x1             0.7966      0.041     19.266      0.000       0.713       0.880\n","x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n","==============================================================================\n","Omnibus:                       14.677   Durbin-Watson:                   1.257\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n","Skew:                          -0.939   Prob(JB):                     2.54e-05\n","Kurtosis:                       5.575   Cond. No.                     5.32e+05\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 5.32e+05. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["X_opt = X[:, [0, 1, 3]]\n","regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n","print(regression_OLS.summary())"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.947\n","Model:                            OLS   Adj. R-squared:                  0.945\n","Method:                 Least Squares   F-statistic:                     849.8\n","Date:                Wed, 13 Sep 2023   Prob (F-statistic):           3.50e-32\n","Time:                        23:02:28   Log-Likelihood:                -527.44\n","No. Observations:                  50   AIC:                             1059.\n","Df Residuals:                      48   BIC:                             1063.\n","Df Model:                           1                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n","x1             0.8543      0.029     29.151      0.000       0.795       0.913\n","==============================================================================\n","Omnibus:                       13.727   Durbin-Watson:                   1.116\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n","Skew:                          -0.911   Prob(JB):                     9.44e-05\n","Kurtosis:                       5.361   Cond. No.                     1.65e+05\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.65e+05. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["X_opt = X[:, [0, 1]]\n","regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n","print(regression_OLS.summary())"]},{"cell_type":"markdown","metadata":{},"source":["CON ESTE NUEVO X_OPT QUE SE OBTIENE SE PROCEDE AL ENTRENAMIENTO"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   prediction       test\n","0   104667.28  103282.38\n","1   134150.83  144259.40\n","2   135207.80  146121.95\n","3    72170.54   77798.83\n","4   179090.59  191050.39\n","5   109824.77  105008.31\n","6    65644.28   81229.06\n","7   100481.43   97483.56\n","8   111431.75  110352.25\n","9   169438.15  166187.94\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_opt, y, test_size = 0.2, random_state = 0)\n","\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","\n","y_pred = regressor.predict(X_test)\n","\n","pd.set_option('display.precision', 2)\n","# ndarray.reshape(filas, col)\n","print(pd.DataFrame(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1), columns=[\"prediction\", \"test\"]))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPhYhte6t7H4wEK4xPpDWT7","name":"Multiple Linear Regression","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
